#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AlphaSeekerç³»ç»Ÿé™æ€åˆ†æå’Œæ¨¡æ‹Ÿæµ‹è¯•è„šæœ¬
"""

import time
import json
import yaml
import psutil
import sys
import os
import subprocess
from datetime import datetime
from typing import Dict, List, Any, Optional
import traceback

class AlphaSeekerStaticTestSuite:
    """AlphaSeekerç³»ç»Ÿé™æ€æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.start_time = datetime.now()
        self.config = self.load_config()
        self.test_results = {
            'system_startup': [],
            'performance_benchmarks': [],
            'component_integration': [],
            'stability_tests': [],
            'file_structure': []
        }
        
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open('/workspace/code/config/main_config.yaml', 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"é…ç½®åŠ è½½å¤±è´¥: {e}")
            return {}
    
    def log_result(self, category: str, test_name: str, status: str, 
                   details: Dict[str, Any], duration: float = 0):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        self.test_results[category].append({
            'test_name': test_name,
            'status': status,
            'details': details,
            'duration': duration,
            'timestamp': datetime.now().isoformat()
        })
        status_emoji = {
            'PASS': 'âœ…',
            'FAIL': 'âŒ',
            'WARN': 'âš ï¸',
            'ERROR': 'ğŸš«'
        }.get(status, 'â“')
        print(f"[{status}] {test_name} - è€—æ—¶: {duration:.2f}s")
    
    def get_system_resources(self):
        """è·å–ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        return {
            'cpu_percent': psutil.cpu_percent(interval=1),
            'memory_percent': psutil.virtual_memory().percent,
            'memory_used_mb': psutil.virtual_memory().used / 1024 / 1024,
            'disk_percent': psutil.disk_usage('/').percent,
            'process_count': len(psutil.pids())
        }
    
    def check_file_structure(self):
        """æ£€æŸ¥æ–‡ä»¶ç»“æ„"""
        print("\n=== æ–‡ä»¶ç»“æ„æ£€æŸ¥ ===")
        
        start_time = time.time()
        
        # æ£€æŸ¥æ ¸å¿ƒæ–‡ä»¶å’Œç›®å½•
        required_files = [
            '/workspace/code/main_integration.py',
            '/workspace/code/requirements.txt',
            '/workspace/code/config/main_config.yaml',
            '/workspace/code/start.sh',
            '/workspace/code/stop.sh'
        ]
        
        required_dirs = [
            '/workspace/code/integrated_api',
            '/workspace/code/ml_engine',
            '/workspace/code/pipeline',
            '/workspace/code/scanner',
            '/workspace/code/validation',
            '/workspace/code/logs',
            '/workspace/code/data',
            '/workspace/code/models'
        ]
        
        missing_files = []
        missing_dirs = []
        
        for file_path in required_files:
            if not os.path.exists(file_path):
                missing_files.append(file_path)
        
        for dir_path in required_dirs:
            if not os.path.exists(dir_path):
                missing_dirs.append(dir_path)
        
        duration = time.time() - start_time
        
        if not missing_files and not missing_dirs:
            self.log_result('file_structure', 'æ ¸å¿ƒæ–‡ä»¶ç»“æ„æ£€æŸ¥', 'PASS', 
                          {'files_ok': len(required_files), 'dirs_ok': len(required_dirs)}, duration)
        else:
            self.log_result('file_structure', 'æ ¸å¿ƒæ–‡ä»¶ç»“æ„æ£€æŸ¥', 'WARN', 
                          {'missing_files': missing_files, 'missing_dirs': missing_dirs}, duration)
    
    def test_system_startup(self):
        """1. ç³»ç»Ÿå¯åŠ¨æµ‹è¯•"""
        print("\n=== 1. ç³»ç»Ÿå¯åŠ¨æµ‹è¯• ===")
        
        # æµ‹è¯•é…ç½®æ–‡ä»¶è§£æ
        start_time = time.time()
        try:
            if self.config and 'components' in self.config:
                duration = time.time() - start_time
                components = list(self.config['components'].keys())
                self.log_result('system_startup', 'é…ç½®æ–‡ä»¶è§£ææµ‹è¯•', 'PASS', 
                              {'components': components, 'config_keys': list(self.config.keys())}, duration)
            else:
                duration = time.time() - start_time
                self.log_result('system_startup', 'é…ç½®æ–‡ä»¶è§£ææµ‹è¯•', 'FAIL', 
                              {'error': 'é…ç½®ç»“æ„ä¸æ­£ç¡®'}, duration)
        except Exception as e:
            duration = time.time() - start_time
            self.log_result('system_startup', 'é…ç½®æ–‡ä»¶è§£ææµ‹è¯•', 'ERROR', 
                          {'error': str(e)}, duration)
        
        # æµ‹è¯•Pythonç¯å¢ƒ
        start_time = time.time()
        try:
            python_version = sys.version.split()[0]
            version_ok = tuple(map(int, python_version.split('.')[:2])) >= (3, 8)
            
            duration = time.time() - start_time
            if version_ok:
                self.log_result('system_startup', 'Pythonç¯å¢ƒæ£€æŸ¥', 'PASS', 
                              {'python_version': python_version}, duration)
            else:
                self.log_result('system_startup', 'Pythonç¯å¢ƒæ£€æŸ¥', 'WARN', 
                              {'python_version': python_version, 'warning': 'ç‰ˆæœ¬è¿‡ä½'}, duration)
        except Exception as e:
            duration = time.time() - start_time
            self.log_result('system_startup', 'Pythonç¯å¢ƒæ£€æŸ¥', 'ERROR', 
                          {'error': str(e)}, duration)
        
        # æµ‹è¯•ä¾èµ–åŒ…å¯¼å…¥
        start_time = time.time()
        try:
            import fastapi
            import lightgbm
            import ccxt
            import pandas
            import numpy
            
            duration = time.time() - start_time
            packages = {
                'fastapi': fastapi.__version__,
                'lightgbm': lightgbm.__version__,
                'ccxt': ccxt.__version__,
                'pandas': pandas.__version__,
                'numpy': numpy.__version__
            }
            
            self.log_result('system_startup', 'æ ¸å¿ƒä¾èµ–åŒ…å¯¼å…¥æµ‹è¯•', 'PASS', 
                          {'packages': packages}, duration)
        except ImportError as e:
            duration = time.time() - start_time
            self.log_result('system_startup', 'æ ¸å¿ƒä¾èµ–åŒ…å¯¼å…¥æµ‹è¯•', 'FAIL', 
                          {'error': str(e)}, duration)
    
    def test_performance_benchmarks(self):
        """2. æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("\n=== 2. æ€§èƒ½åŸºå‡†æµ‹è¯• ===")
        
        # æµ‹è¯•æ•°æ®å¤„ç†æ€§èƒ½
        start_time = time.time()
        try:
            import numpy as np
            import pandas as pd
            
            # æ¨¡æ‹Ÿæ•°æ®å¤„ç†
            data_size = 10000
            test_data = np.random.randn(data_size, 10)
            df = pd.DataFrame(test_data)
            
            # æ‰§è¡Œä¸€äº›æ•°æ®æ“ä½œ
            result = df.rolling(window=5).mean()
            correlation = df.corr()
            
            duration = time.time() - start_time
            
            if duration < 2.0:  # 2ç§’å†…å®Œæˆ
                self.log_result('performance_benchmarks', 'æ•°æ®å¤„ç†æ€§èƒ½æµ‹è¯•', 'PASS', 
                              {'data_size': data_size, 'duration': duration}, duration)
            else:
                self.log_result('performance_benchmarks', 'æ•°æ®å¤„ç†æ€§èƒ½æµ‹è¯•', 'WARN', 
                              {'data_size': data_size, 'duration': duration}, duration)
        except Exception as e:
            duration = time.time() - start_time
            self.log_result('performance_benchmarks', 'æ•°æ®å¤„ç†æ€§èƒ½æµ‹è¯•', 'ERROR', 
                          {'error': str(e)}, duration)
        
        # æµ‹è¯•æœºå™¨å­¦ä¹ æ¨¡å‹æ€§èƒ½
        start_time = time.time()
        try:
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.datasets import make_classification
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            X, y = make_classification(n_samples=1000, n_features=10, random_state=42)
            
            # è®­ç»ƒæ¨¡å‹
            model = RandomForestClassifier(n_estimators=50, random_state=42)
            model.fit(X, y)
            
            # é¢„æµ‹
            predictions = model.predict(X[:100])
            
            duration = time.time() - start_time
            
            if duration < 1.0:  # 1ç§’å†…å®Œæˆ
                self.log_result('performance_benchmarks', 'æœºå™¨å­¦ä¹ æ€§èƒ½æµ‹è¯•', 'PASS', 
                              {'samples': 1000, 'duration': duration}, duration)
            else:
                self.log_result('performance_benchmarks', 'æœºå™¨å­¦ä¹ æ€§èƒ½æµ‹è¯•', 'WARN', 
                              {'samples': 1000, 'duration': duration}, duration)
        except Exception as e:
            duration = time.time() - start_time
            self.log_result('performance_benchmarks', 'æœºå™¨å­¦ä¹ æ€§èƒ½æµ‹è¯•', 'ERROR', 
                          {'error': str(e)}, duration)
        
        # æµ‹è¯•å¹¶å‘æ€§èƒ½
        start_time = time.time()
        try:
            import concurrent.futures
            
            def cpu_intensive_task(n):
                return sum(i**2 for i in range(n))
            
            # æ¨¡æ‹Ÿå¹¶å‘ä»»åŠ¡
            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                tasks = [executor.submit(cpu_intensive_task, 1000) for _ in range(20)]
                results = [task.result() for task in concurrent.futures.as_completed(tasks)]
            
            duration = time.time() - start_time
            
            if duration < 5.0:  # 5ç§’å†…å®Œæˆ
                self.log_result('performance_benchmarks', 'å¹¶å‘å¤„ç†æ€§èƒ½æµ‹è¯•', 'PASS', 
                              {'tasks': 20, 'duration': duration}, duration)
            else:
                self.log_result('performance_benchmarks', 'å¹¶å‘å¤„ç†æ€§èƒ½æµ‹è¯•', 'WARN', 
                              {'tasks': 20, 'duration': duration}, duration)
        except Exception as e:
            duration = time.time() - start_time
            self.log_result('performance_benchmarks', 'å¹¶å‘å¤„ç†æ€§èƒ½æµ‹è¯•', 'ERROR', 
                          {'error': str(e)}, duration)
    
    def test_component_integration(self):
        """3. ç»„ä»¶é›†æˆæµ‹è¯•"""
        print("\n=== 3. ç»„ä»¶é›†æˆæµ‹è¯• ===")
        
        # æ£€æŸ¥å„ç»„ä»¶æ¨¡å—
        components = [
            'integrated_api',
            'ml_engine', 
            'pipeline',
            'scanner',
            'validation'
        ]
        
        start_time = time.time()
        try:
            available_components = []
            missing_components = []
            
            for component in components:
                component_path = f'/workspace/code/{component}'
                if os.path.exists(component_path) and os.path.isdir(component_path):
                    available_components.append(component)
                else:
                    missing_components.append(component)
            
            duration = time.time() - start_time
            
            if len(available_components) == len(components):
                self.log_result('component_integration', 'ç»„ä»¶ç›®å½•æ£€æŸ¥', 'PASS', 
                              {'available': available_components, 'missing': missing_components}, duration)
            else:
                self.log_result('component_integration', 'ç»„ä»¶ç›®å½•æ£€æŸ¥', 'WARN', 
                              {'available': available_components, 'missing': missing_components}, duration)
        except Exception as e:
            duration = time.time() - start_time
            self.log_result('component_integration', 'ç»„ä»¶ç›®å½•æ£€æŸ¥', 'ERROR', 
                          {'error': str(e)}, duration)
        
        # æ£€æŸ¥ç»„ä»¶é…ç½®æ–‡ä»¶
        start_time = time.time()
        try:
            component_configs = {
                'api': 'integrated_api/requirements.txt',
                'ml_engine': 'ml_engine/requirements.txt',
                'scanner': 'scanner/requirements.txt'
            }
            
            config_status = {}
            for component, config_file in component_configs.items():
                config_path = f'/workspace/code/{config_file}'
                config_status[component] = os.path.exists(config_path)
            
            duration = time.time() - start_time
            
            all_exist = all(config_status.values())
            if all_exist:
                self.log_result('component_integration', 'ç»„ä»¶é…ç½®æ£€æŸ¥', 'PASS', 
                              {'configs': config_status}, duration)
            else:
                self.log_result('component_integration', 'ç»„ä»¶é…ç½®æ£€æŸ¥', 'WARN', 
                              {'configs': config_status}, duration)
        except Exception as e:
            duration = time.time() - start_time
            self.log_result('component_integration', 'ç»„ä»¶é…ç½®æ£€æŸ¥', 'ERROR', 
                          {'error': str(e)}, duration)
    
    def test_stability(self):
        """4. ç¨³å®šæ€§æµ‹è¯•"""
        print("\n=== 4. ç¨³å®šæ€§æµ‹è¯• ===")
        
        # æµ‹è¯•ç³»ç»Ÿèµ„æºç›‘æ§
        start_time = time.time()
        try:
            resources_before = self.get_system_resources()
            
            # æ¨¡æ‹Ÿè´Ÿè½½
            import numpy as np
            data = np.random.randn(1000, 100)
            result = np.sum(data ** 2, axis=1)
            
            resources_after = self.get_system_resources()
            duration = time.time() - start_time
            
            memory_increase = resources_after['memory_used_mb'] - resources_before['memory_used_mb']
            
            if memory_increase < 100:  # å†…å­˜å¢åŠ å°äº100MB
                self.log_result('stability_tests', 'å†…å­˜ä½¿ç”¨ç¨³å®šæ€§æµ‹è¯•', 'PASS', 
                              {'memory_increase_mb': memory_increase, 'duration': duration}, duration)
            else:
                self.log_result('stability_tests', 'å†…å­˜ä½¿ç”¨ç¨³å®šæ€§æµ‹è¯•', 'WARN', 
                              {'memory_increase_mb': memory_increase, 'duration': duration}, duration)
        except Exception as e:
            duration = time.time() - start_time
            self.log_result('stability_tests', 'å†…å­˜ä½¿ç”¨ç¨³å®šæ€§æµ‹è¯•', 'ERROR', 
                          {'error': str(e)}, duration)
        
        # æµ‹è¯•å¼‚å¸¸å¤„ç†
        start_time = time.time()
        try:
            def test_function(should_fail=False):
                if should_fail:
                    raise ValueError("æµ‹è¯•å¼‚å¸¸")
                return "success"
            
            # æµ‹è¯•æ­£å¸¸æƒ…å†µ
            result1 = test_function(False)
            
            # æµ‹è¯•å¼‚å¸¸æƒ…å†µ
            try:
                result2 = test_function(True)
                exception_handled = False
            except ValueError:
                exception_handled = True
            
            duration = time.time() - start_time
            
            if result1 == "success" and exception_handled:
                self.log_result('stability_tests', 'å¼‚å¸¸å¤„ç†æµ‹è¯•', 'PASS', 
                              {'normal_case': result1, 'exception_handled': exception_handled}, duration)
            else:
                self.log_result('stability_tests', 'å¼‚å¸¸å¤„ç†æµ‹è¯•', 'FAIL', 
                              {'normal_case': result1, 'exception_handled': exception_handled}, duration)
        except Exception as e:
            duration = time.time() - start_time
            self.log_result('stability_tests', 'å¼‚å¸¸å¤„ç†æµ‹è¯•', 'ERROR', 
                          {'error': str(e)}, duration)
    
    def test_configuration_validation(self):
        """5. é…ç½®éªŒè¯æµ‹è¯•"""
        print("\n=== 5. é…ç½®éªŒè¯æµ‹è¯• ===")
        
        start_time = time.time()
        try:
            config_validation = {
                'server_config': 'server' in self.config,
                'components_config': 'components' in self.config,
                'performance_config': 'performance' in self.config,
                'logging_config': 'logging' in self.config,
                'paths_config': 'paths' in self.config
            }
            
            required_configs = ['server', 'components', 'performance', 'logging', 'paths']
            missing_configs = [key for key in required_configs if key not in self.config]
            
            duration = time.time() - start_time
            
            if not missing_configs:
                self.log_result('component_integration', 'é…ç½®ç»“æ„éªŒè¯', 'PASS', 
                              {'validation': config_validation}, duration)
            else:
                self.log_result('component_integration', 'é…ç½®ç»“æ„éªŒè¯', 'WARN', 
                              {'validation': config_validation, 'missing': missing_configs}, duration)
        except Exception as e:
            duration = time.time() - start_time
            self.log_result('component_integration', 'é…ç½®ç»“æ„éªŒè¯', 'ERROR', 
                          {'error': str(e)}, duration)
    
    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        total_duration = (datetime.now() - self.start_time).total_seconds()
        
        # ç»Ÿè®¡å„ç±»æµ‹è¯•ç»“æœ
        summary = {}
        for category, tests in self.test_results.items():
            passed = len([t for t in tests if t['status'] == 'PASS'])
            failed = len([t for t in tests if t['status'] == 'FAIL'])
            warnings = len([t for t in tests if t['status'] == 'WARN'])
            errors = len([t for t in tests if t['status'] == 'ERROR'])
            
            summary[category] = {
                'total': len(tests),
                'passed': passed,
                'failed': failed,
                'warnings': warnings,
                'errors': errors,
                'success_rate': (passed / len(tests) * 100) if tests else 0
            }
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_content = f"""# AlphaSeekerç³»ç»Ÿç¨³å®šæ€§å’Œæ€§èƒ½æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ¦‚è¿°
- **æµ‹è¯•æ—¶é—´**: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **æ€»æµ‹è¯•æ—¶é•¿**: {total_duration:.2f}ç§’
- **æµ‹è¯•ç¯å¢ƒ**: Python {sys.version.split()[0]}
- **ç³»ç»Ÿèµ„æº**: CPU {psutil.cpu_count()}æ ¸å¿ƒ, å†…å­˜ {psutil.virtual_memory().total / 1024 / 1024 / 1024:.1f}GB
- **æµ‹è¯•ç±»å‹**: é™æ€åˆ†æ + æ€§èƒ½æ¨¡æ‹Ÿ

## æµ‹è¯•ç»“æœæ±‡æ€»

"""
        
        for category, stats in summary.items():
            category_name = {
                'file_structure': 'æ–‡ä»¶ç»“æ„æ£€æŸ¥',
                'system_startup': 'ç³»ç»Ÿå¯åŠ¨æµ‹è¯•',
                'performance_benchmarks': 'æ€§èƒ½åŸºå‡†æµ‹è¯•',
                'component_integration': 'ç»„ä»¶é›†æˆæµ‹è¯•',
                'stability_tests': 'ç¨³å®šæ€§æµ‹è¯•'
            }.get(category, category)
            
            status_emoji = "âœ…" if stats['success_rate'] >= 90 else "âš ï¸" if stats['success_rate'] >= 70 else "âŒ"
            
            report_content += f"""### {status_emoji} {category_name}
- **æ€»æµ‹è¯•æ•°**: {stats['total']}
- **é€šè¿‡**: {stats['passed']} ({stats['success_rate']:.1f}%)
- **å¤±è´¥**: {stats['failed']}
- **è­¦å‘Š**: {stats['warnings']}
- **é”™è¯¯**: {stats['errors']}

"""

        report_content += "\n## è¯¦ç»†æµ‹è¯•ç»“æœ\n\n"
        
        for category, tests in self.test_results.items():
            category_name = {
                'file_structure': 'æ–‡ä»¶ç»“æ„æ£€æŸ¥',
                'system_startup': 'ç³»ç»Ÿå¯åŠ¨æµ‹è¯•',
                'performance_benchmarks': 'æ€§èƒ½åŸºå‡†æµ‹è¯•',
                'component_integration': 'ç»„ä»¶é›†æˆæµ‹è¯•',
                'stability_tests': 'ç¨³å®šæ€§æµ‹è¯•'
            }.get(category, category)
            
            report_content += f"### {category_name}\n\n"
            
            for test in tests:
                status_emoji = {
                    'PASS': 'âœ…',
                    'FAIL': 'âŒ',
                    'WARN': 'âš ï¸',
                    'ERROR': 'ğŸš«'
                }.get(test['status'], 'â“')
                
                report_content += f"- **{status_emoji} {test['test_name']}** ({test['duration']:.3f}s)\n"
                report_content += f"  - çŠ¶æ€: {test['status']}\n"
                report_content += f"  - è¯¦æƒ…: {json.dumps(test['details'], indent=6, ensure_ascii=False)}\n\n"
        
        # æ€§èƒ½æŒ‡æ ‡æ€»ç»“
        performance_tests = [t for t in self.test_results['performance_benchmarks'] if t['status'] == 'PASS']
        if performance_tests:
            report_content += "\n## æ€§èƒ½æŒ‡æ ‡æ€»ç»“\n\n"
            report_content += "### åŸºå‡†æ€§èƒ½æŒ‡æ ‡\n\n"
            for test in performance_tests:
                report_content += f"- **{test['test_name']}**: {test['duration']:.3f}ç§’\n"
                if 'data_size' in test['details']:
                    report_content += f"  - æ•°æ®è§„æ¨¡: {test['details']['data_size']:,}\n"
                if 'samples' in test['details']:
                    report_content += f"  - æ ·æœ¬æ•°é‡: {test['details']['samples']:,}\n"
                if 'tasks' in test['details']:
                    report_content += f"  - å¹¶å‘ä»»åŠ¡: {test['details']['tasks']}\n"
                report_content += "\n"
        
        # ç³»ç»Ÿèµ„æºä¿¡æ¯
        resources = self.get_system_resources()
        report_content += "\n## ç³»ç»Ÿèµ„æºä¿¡æ¯\n\n"
        report_content += f"- **CPUæ ¸å¿ƒæ•°**: {psutil.cpu_count()}\n"
        report_content += f"- **CPUä½¿ç”¨ç‡**: {resources['cpu_percent']:.1f}%\n"
        report_content += f"- **å†…å­˜ä½¿ç”¨ç‡**: {resources['memory_percent']:.1f}%\n"
        report_content += f"- **å†…å­˜ä½¿ç”¨é‡**: {resources['memory_used_mb']:.1f} MB\n"
        report_content += f"- **ç£ç›˜ä½¿ç”¨ç‡**: {resources['disk_percent']:.1f}%\n"
        report_content += f"- **è¿›ç¨‹æ•°é‡**: {resources['process_count']}\n\n"
        
        # é…ç½®æ–‡ä»¶åˆ†æ
        if self.config:
            report_content += "\n## é…ç½®åˆ†æ\n\n"
            report_content += "### æ€§èƒ½é…ç½®\n"
            if 'performance' in self.config:
                perf_config = self.config['performance']
                if 'max_concurrent_tasks' in perf_config:
                    report_content += f"- **æœ€å¤§å¹¶å‘ä»»åŠ¡**: {perf_config['max_concurrent_tasks']}\n"
                if 'request_timeout' in perf_config:
                    report_content += f"- **è¯·æ±‚è¶…æ—¶**: {perf_config['request_timeout']}ç§’\n"
                if 'batch_size' in perf_config:
                    report_content += f"- **æ‰¹å¤„ç†å¤§å°**: {perf_config['batch_size']}\n"
            
            report_content += "\n### ç»„ä»¶é…ç½®\n"
            if 'components' in self.config:
                components = self.config['components']
                for component_name, component_config in components.items():
                    report_content += f"- **{component_name}**: é…ç½®å®Œæ•´\n"
            
            report_content += "\n"
        
        # å»ºè®®å’Œæ”¹è¿›
        total_tests = sum(stats['total'] for stats in summary.values())
        total_passed = sum(stats['passed'] for stats in summary.values())
        overall_success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
        
        report_content += "\n## ç³»ç»Ÿè¯„ä¼°å’Œå»ºè®®\n\n"
        
        if overall_success_rate >= 90:
            report_content += "### âœ… ç³»ç»ŸçŠ¶æ€ï¼šä¼˜ç§€\n"
            report_content += "ç³»ç»Ÿæ•´ä½“æ€§èƒ½è¡¨ç°ä¼˜ç§€ï¼Œå¤§éƒ¨åˆ†æµ‹è¯•é€šè¿‡ã€‚\n\n"
        elif overall_success_rate >= 70:
            report_content += "### âš ï¸ ç³»ç»ŸçŠ¶æ€ï¼šè‰¯å¥½\n"
            report_content += "ç³»ç»Ÿæ•´ä½“æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œå­˜åœ¨ä¸€äº›éœ€è¦å…³æ³¨çš„é—®é¢˜ã€‚\n\n"
        else:
            report_content += "### âŒ ç³»ç»ŸçŠ¶æ€ï¼šéœ€è¦æ”¹è¿›\n"
            report_content += "ç³»ç»Ÿå­˜åœ¨è¾ƒå¤šé—®é¢˜ï¼Œéœ€è¦ç´§æ€¥ä¿®å¤å’Œä¼˜åŒ–ã€‚\n\n"
        
        report_content += "### å»ºè®®æ”¹è¿›æªæ–½\n\n"
        report_content += "1. **æ€§èƒ½ä¼˜åŒ–**\n"
        report_content += "   - ä¼˜åŒ–æ•°æ®å¤„ç†ç®—æ³•\n"
        report_content += "   - æ”¹è¿›æœºå™¨å­¦ä¹ æ¨¡å‹æ¨ç†é€Ÿåº¦\n"
        report_content += "   - ä¼˜åŒ–å¹¶å‘å¤„ç†æœºåˆ¶\n\n"
        
        report_content += "2. **ç¨³å®šæ€§å¢å¼º**\n"
        report_content += "   - åŠ å¼ºå¼‚å¸¸å¤„ç†æœºåˆ¶\n"
        report_content += "   - å®æ–½èµ„æºä½¿ç”¨ç›‘æ§\n"
        report_content += "   - æ·»åŠ è‡ªåŠ¨æ¢å¤æœºåˆ¶\n\n"
        
        report_content += "3. **ç›‘æ§å’Œå‘Šè­¦**\n"
        report_content += "   - å®æ–½å®æ—¶æ€§èƒ½ç›‘æ§\n"
        report_content += "   - è®¾ç½®æ€§èƒ½å‘Šè­¦é˜ˆå€¼\n"
        report_content += "   - å»ºç«‹æ—¥å¿—åˆ†æç³»ç»Ÿ\n\n"
        
        report_content += "4. **éƒ¨ç½²å’Œè¿ç»´**\n"
        report_content += "   - ä¼˜åŒ–Dockerå®¹å™¨é…ç½®\n"
        report_content += "   - å®æ–½è´Ÿè½½å‡è¡¡\n"
        report_content += "   - å»ºç«‹è‡ªåŠ¨åŒ–æµ‹è¯•æµç¨‹\n\n"
        
        report_content += f"---\n*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"
        report_content += f"*æµ‹è¯•å·¥å…·: AlphaSeeker Static Test Suite v1.0*\n"
        
        return report_content
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹AlphaSeekerç³»ç»Ÿç¨³å®šæ€§å’Œæ€§èƒ½æµ‹è¯•\n")
        
        try:
            # è®°å½•æµ‹è¯•å¼€å§‹æ—¶çš„ç³»ç»Ÿèµ„æº
            initial_resources = self.get_system_resources()
            print(f"åˆå§‹ç³»ç»Ÿèµ„æº - CPU: {initial_resources['cpu_percent']:.1f}%, "
                  f"å†…å­˜: {initial_resources['memory_percent']:.1f}%\n")
            
            # æ‰§è¡Œå„é¡¹æµ‹è¯•
            self.check_file_structure()
            self.test_system_startup()
            self.test_performance_benchmarks()
            self.test_component_integration()
            self.test_stability()
            self.test_configuration_validation()
            
            # è®°å½•æµ‹è¯•ç»“æŸæ—¶çš„ç³»ç»Ÿèµ„æº
            final_resources = self.get_system_resources()
            print(f"\næµ‹è¯•ç»“æŸç³»ç»Ÿèµ„æº - CPU: {final_resources['cpu_percent']:.1f}%, "
                  f"å†…å­˜: {final_resources['memory_percent']:.1f}%")
            
            # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
            report_content = self.generate_report()
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs('/workspace/test_results', exist_ok=True)
            
            # ä¿å­˜æŠ¥å‘Š
            with open('/workspace/test_results/system_performance_test.md', 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            print(f"\nğŸ“Š æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: /workspace/test_results/system_performance_test.md")
            print(f"ğŸ“ˆ æµ‹è¯•å®Œæˆï¼Œæ€»è€—æ—¶: {(datetime.now() - self.start_time).total_seconds():.2f}ç§’")
            
            # è¾“å‡ºç®€è¦ç»“æœ
            total_tests = sum(len(tests) for tests in self.test_results.values())
            total_passed = sum(len([t for t in tests if t['status'] == 'PASS']) for tests in self.test_results.values())
            success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
            
            print(f"\nğŸ“‹ æµ‹è¯•æ€»ç»“: {total_passed}/{total_tests} é€šè¿‡ ({success_rate:.1f}%)")
            
        except Exception as e:
            print(f"æµ‹è¯•æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    try:
        test_suite = AlphaSeekerStaticTestSuite()
        test_suite.run_all_tests()
    except Exception as e:
        print(f"æµ‹è¯•å¯åŠ¨å¤±è´¥: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    main()